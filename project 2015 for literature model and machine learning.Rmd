---
title: '2015 brfss analysis'
author: "Qi Lu"
date: "2025"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load the package

```{r}

library(glmnet)
library(caret)
library(MASS)
library(rpart)
library(rpart.plot)
library(lightgbm)
library(pROC)
library(xgboost)
library(shapviz)
library(DMwR)
library(ggplot2)
library(lattice)
library(dplyr)

#To use SMOTE, we need to download the DMwR package from https://cran.r-project.org/src/contrib/Archive/DMwR/ and manually add it to R, as this package is no longer available in the official R repository. Make sure to use the latest version, 0.4.1.
```
#This file is main on machine learning methods , only contain few logistic best models.

#Please ensure you rerun the cleandataset function each time before executing the full logistic regression procedure document; #otherwise, the results obtained here may be misleading due to data contamination or unintended carryover effects.

```{r}

setwd("C:/Users/LU QI/Desktop")

X2015 <- read.csv("2015.csv", check.names = FALSE)
```



```{r}
# Select relevant columns
clean_dataset <- X2015[, c("SEX", "_AGE80", "WEIGHT2", "HEIGHT3", "_RFSMOK3", "_RFBING5", "INCOME2",
                           "EXERANY2", "CHCKIDNY", "HAVARTH3", "TOLDHI2", "_RFHYPE5", "CVDSTRK3",
                           "ASTHMA3", "MENTHLTH", "_RACE", "EDUCA", "FRUIT1", "FVBEANS", "FVGREEN",
                           "STRENGTH", "_MICHD")]

# Filter rows based on value constraints 
clean_dataset <- subset(clean_dataset, 
                        `_MICHD` %in% c("1", "2") &
                        WEIGHT2 >= 0 & WEIGHT2 <= 999 &
                        HEIGHT3 >= 200 & HEIGHT3 <= 711 &  # Keep only 200-711 for height
                        `_RFSMOK3` %in% c("1", "2") &
                        `_RFBING5` %in% c("1", "2") &
                        INCOME2 %in% c("1", "2", "3", "4", "5", "6", "7", "8") &
                        EXERANY2 %in% c("1", "2") &
                        CHCKIDNY %in% c("1", "2") &
                        HAVARTH3 %in% c("1", "2") &
                        TOLDHI2 %in% c("1", "2") &
                        `_RFHYPE5` %in% c("1", "2") &
                        CVDSTRK3 %in% c("1", "2") &
                        ASTHMA3 %in% c("1", "2") &
                        (`MENTHLTH` >= 1 & `MENTHLTH` <= 30 | `MENTHLTH` == 88) &
                        `_RACE` %in% c("1", "2", "3", "4", "5", "6", "7", "8") &
                        EDUCA %in% c("1", "2", "3", "4", "5", "6") &
                        !(FRUIT1 %in% c(777, 999)) & !is.na(FRUIT1) &
                        !(FVBEANS %in% c(777, 999)) & !is.na(FVBEANS) &
                        !(FVGREEN %in% c(777, 999)) & !is.na(FVGREEN) &
                        !(STRENGTH %in% c(777, 999)) & !is.na(STRENGTH))

# Update column names
names(clean_dataset) <- c("SEX", "AGE", "WEIGHT", "HEIGHT", "SMOK", "DRINK", "INCOME",
                          "EXERCISE", "KIDNEY", "ARTHRITIS", "HIGH_CHOLESTEROL", "HIGH_BLOOD_PRESSURE",
                          "STROKE", "ASTHMA", "MENTAL", "RACE", "EDUCATION", "FRUIT", "BEANS",
                          "DARK_GREEN_VEG", "MUSCLES_EXERCISE", "y")

# Convert HEIGHT3 from feet/inches to centimeters and round to integer
convert_height_to_cm <- function(height) {
  feet <- as.integer(height / 100)  # Extract feet
  inches <- height %% 100           # Extract inches
  return(as.integer(round(feet * 30.48 + inches * 2.54)))  # Convert and round to integer
}

# Apply the conversion function to HEIGHT
clean_dataset$HEIGHT <- sapply(clean_dataset$HEIGHT, convert_height_to_cm)

# Convert _MICHD to binary factor (1: yes, 0: no)
clean_dataset$y <- factor(ifelse(clean_dataset$y == 2, 0, 1), labels = c("no", "yes"))

# Convert categorical variables to factors with meaningful labels
clean_dataset$SEX <- factor(clean_dataset$SEX, levels = c(1, 2), labels = c("MALE", "FEMALE"))
clean_dataset$SMOK <- factor(clean_dataset$SMOK, levels = c(1, 2), labels = c("_NO", "_YES"))
clean_dataset$DRINK <- factor(clean_dataset$DRINK, levels = c(1, 2), labels = c("_NO", "_YES"))
clean_dataset$EXERCISE <- factor(clean_dataset$EXERCISE, levels = c(1, 2), labels = c("_YES", "_NO"))
clean_dataset$KIDNEY <- factor(clean_dataset$KIDNEY, levels = c(1, 2), labels = c("_YES", "_NO"))
clean_dataset$ARTHRITIS <- factor(clean_dataset$ARTHRITIS, levels = c(1, 2), labels = c("_YES", "_NO"))
clean_dataset$HIGH_CHOLESTEROL <- factor(clean_dataset$HIGH_CHOLESTEROL, levels = c(1, 2), labels = c("_YES", "_NO"))
clean_dataset$HIGH_BLOOD_PRESSURE <- factor(clean_dataset$HIGH_BLOOD_PRESSURE, levels = c(1, 2), labels = c("_NO", "_YES"))
clean_dataset$STROKE <- factor(clean_dataset$STROKE, levels = c(1, 2), labels = c("_YES", "_NO"))
clean_dataset$ASTHMA <- factor(clean_dataset$ASTHMA, levels = c(1, 2), labels = c("_YES", "_NO"))

clean_dataset$INCOME <- factor(clean_dataset$INCOME, levels = 1:8,
                                labels = c("less than $20,000", "less than $20,000", "less than $20,000",
                                           "$20,000-$35,000", "$20,000-$35,000",
                                           "$50,000-$75,000", "$50,000-$75,000", "$75,000 or more"))
clean_dataset$RACE <- factor(clean_dataset$RACE, levels = 1:8, 
                             labels = c("White", "Black", "Indigenous", "Asian-Pacific", "Asian-Pacific",
                                        "Other race", "Other race", "Hispanic"))
clean_dataset$EDUCATION <- factor(clean_dataset$EDUCATION, levels = 1:6,
                                  labels = c("Never", "Elementary", "High school", "High school",
                                             "College or technical school", "College or technical school"))

# Clean numerical variables with specific transformations
clean_dataset$MUSCLES_EXERCISE[clean_dataset$MUSCLES_EXERCISE == 888] <- 0
clean_dataset$MUSCLES_EXERCISE <- with(clean_dataset, ifelse(MUSCLES_EXERCISE >= 101 & MUSCLES_EXERCISE <= 199,
                                                             (MUSCLES_EXERCISE - 100) * 4,
                                                             ifelse(MUSCLES_EXERCISE >= 201 & MUSCLES_EXERCISE <= 299,
                                                                    MUSCLES_EXERCISE - 200, MUSCLES_EXERCISE)))
clean_dataset <- subset(clean_dataset, MUSCLES_EXERCISE <= 90)

convert_to_monthly <- function(var) {
  var[var == 555 | var == 300] <- 0
  var[var >= 101 & var <= 199] <- (var[var >= 101 & var <= 199] - 100) * 30
  var[var >= 201 & var <= 299] <- (var[var >= 201 & var <= 299] - 200) * 4.33
  var[var >= 301 & var <= 399] <- var[var >= 301 & var <= 399] - 300
  return(as.integer(var))
}

clean_dataset$FRUIT <- convert_to_monthly(clean_dataset$FRUIT)
clean_dataset <- subset(clean_dataset, FRUIT <= 200)

clean_dataset$DARK_GREEN_VEG <- convert_to_monthly(clean_dataset$DARK_GREEN_VEG)
clean_dataset <- subset(clean_dataset, DARK_GREEN_VEG <= 150)

clean_dataset$BEANS <- convert_to_monthly(clean_dataset$BEANS)
clean_dataset$MENTAL <- ifelse(clean_dataset$MENTAL == 88, 0, clean_dataset$MENTAL)


```



#test na value
```{r}
colSums(is.na(clean_dataset))

```
```{r}
# Cook's Distance 
cooks_d <- cooks.distance(big_model)

# Check the first few values
head(cooks_d)
plot(cooks_d, main="Cook's Distance", ylab="Cook's Distance", xlab="Observation Index")
abline(h = 0.001, col = "red")  # Add a threshold line for Cook's Distance


# selected numerical variables
numerical_vars <- c("AGE", "WEIGHT", "HEIGHT", "MENTAL",
                    "FRUIT", "BEANS", "DARK_GREEN_VEG", "MUSCLES_EXERCISE")

# plot
for (i in seq(1, length(numerical_vars), by = 2)) {
  
  par(mfrow = c(1, 2))  
  
  var1 <- numerical_vars[i]
  var2 <- numerical_vars[i + 1]
  
  # first plot
  png(paste0("iqr_plot_", i, "_", i+1, ".png"), width = 800, height = 400)
  dev.off()
  boxplot(clean_dataset[[var1]],
          main = var1,
          col = "lightblue",
          border = "darkblue",
          outcol = "red",
          ylab = "Value")
  
  # second plot
  boxplot(clean_dataset[[var2]],
          main = var2,
          col = "lightblue",
          border = "darkblue",
          outcol = "red",
          ylab = "Value")
}

```



```{r}
  

sampled_data <- clean_dataset


```


```{r}

table(sampled_data$y)

```

#Data set division

```{r}

set.seed(2025)  # Set random seed

# Total number of rows in the dataset
n <- nrow(sampled_data)

# Indices for training set samples
train_indices <- sample(1:n, size = 0.7 * n)

# Split the dataset
train_data <- sampled_data[train_indices, ]  # Training set
test_data <- sampled_data[-train_indices, ]  # Test set

```

#use smote
```{r}
#before smote
print(table(train_data$y))
```


```{r}
# Perform oversampling using SMOTE
set.seed(2025)
train_data_smote <- SMOTE(y ~ ., data = train_data, perc.over = 300, perc.under = 100)
#train_data_smote <- train_data

# Check the class distribution after oversampling
table(train_data_smote$y)
```


#logistic regression without interaction
```{r}
# Big model without interactions
big_model_no_interaction <- glm(
  y ~ SEX + AGE + WEIGHT + HEIGHT + RACE + EDUCATION + INCOME +
       FRUIT + DARK_GREEN_VEG + MUSCLES_EXERCISE + DRINK + SMOK + EXERCISE + BEANS +
       HIGH_BLOOD_PRESSURE + ASTHMA + STROKE + KIDNEY + HIGH_CHOLESTEROL + MENTAL + ARTHRITIS,
  family = binomial(link = "logit"),
  data = train_data
)
summary(big_model_no_interaction)

```






                            
#interaction
```{r}
#  big model with interaction
big_model_i <- glm(
  y ~ SEX + AGE + WEIGHT + HEIGHT + RACE + EDUCATION + INCOME + 
       FRUIT + DARK_GREEN_VEG + MUSCLES_EXERCISE + DRINK + SMOK + EXERCISE + BEANS +
       HIGH_BLOOD_PRESSURE + ASTHMA + STROKE + KIDNEY + HIGH_CHOLESTEROL + MENTAL + ARTHRITIS +
       AGE:WEIGHT + AGE:INCOME + 
       HIGH_BLOOD_PRESSURE:STROKE + 
       SEX:SMOK +  WEIGHT:EXERCISE,
  family = binomial(link = "logit"),
  data = train_data
)

summary(big_model_i)


```



#now use smote
```{r}
# Big model without interactions
big_model_no_interaction_s <- glm(
  y ~ SEX + AGE + WEIGHT + HEIGHT + RACE + EDUCATION + INCOME +
       FRUIT + DARK_GREEN_VEG + MUSCLES_EXERCISE + DRINK + SMOK + EXERCISE + BEANS +
       HIGH_BLOOD_PRESSURE + ASTHMA + STROKE + KIDNEY + HIGH_CHOLESTEROL + MENTAL + ARTHRITIS,
  family = binomial(link = "logit"),
  data = train_data_smote
)
summary(big_model_no_interaction_s)
```

```{r}
# simplied big model with interaction
big_model_i_s <- glm(
  y ~ SEX + AGE + WEIGHT + HEIGHT + RACE + EDUCATION + INCOME + 
       FRUIT + DARK_GREEN_VEG + MUSCLES_EXERCISE + DRINK + SMOK + EXERCISE + BEANS +
       HIGH_BLOOD_PRESSURE + ASTHMA + STROKE + KIDNEY + HIGH_CHOLESTEROL + MENTAL + ARTHRITIS +
       AGE:WEIGHT + AGE:INCOME + 
       HIGH_BLOOD_PRESSURE:STROKE + 
       SEX:SMOK +  WEIGHT:EXERCISE,
  family = binomial(link = "logit"),
  data = train_data_smote
)

summary(big_model_i_s)
```



#Decision tree 

```{r}

# Decision tree parameter grid
tree_grid <- expand.grid(cp = seq(0.001, 0.1, by = 0.01))

# Decision tree cross-validation
set.seed(123)
tree_tune <- train(
  y ~ ., data = train_data,
  method = "rpart",
  tuneGrid = tree_grid,
  trControl = trainControl(method = "cv", number = 5)
)

# Display the best parameter
print(tree_tune$bestTune)

# visualization
tree_model <- tree_tune$finalModel
rpart.plot(tree_model, type = 2)





```

#use smote
```{r}
# Decision tree parameter grid
tree_grid_s <- expand.grid(cp = seq(0.001, 0.1, by = 0.01))

# Decision tree cross-validation
set.seed(123)
tree_tune_s <- train(
  y ~ ., data = train_data_smote,
  method = "rpart",
  tuneGrid = tree_grid_s,
  trControl = trainControl(method = "cv", number = 5)
)

# Display the best parameter
print(tree_tune_s$bestTune)

# visualization
tree_model_s <- tree_tune_s$finalModel
rpart.plot(tree_model_s, type = 2)

```


#Random forest
```{r}

# Random forest parameter grid
rf_grid <- expand.grid(mtry = c(2, 3, 4, 5))

# Random forest cross-validation
set.seed(123)
rf_tune <- train(
  y ~ ., data = train_data,
  method = "rf",
  tuneGrid = rf_grid,
  trControl = trainControl(method = "cv", number = 3),
  ntree = 100
)

# View the best parameters
print(rf_tune$bestTune)


```




```{r}
# Calculate variable importance using the trained random forest model
rf_var_imp <- varImp(rf_tune, scale = FALSE)

# Plot variable importance using the built-in plot method
plot(rf_var_imp, main = "Variable Importance - Random Forest")

# Alternatively, using ggplot2 for a more customized visualization
library(ggplot2)
var_imp_df <- data.frame(
  Variable = rownames(rf_var_imp$importance),
  Importance = rf_var_imp$importance[, 1]
)

ggplot(var_imp_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(x = "Variables", y = "Importance", title = "Variable Importance - Random Forest") +
  theme_minimal()

```


#use smote
```{r}
# Random forest parameter grid
rf_grid_s <- expand.grid(mtry = c(2, 3, 4, 5))

# Random forest cross-validation
set.seed(123)
rf_tune_s <- train(
  y ~ ., data = train_data_smote,
  method = "rf",
  tuneGrid = rf_grid_s,
  trControl = trainControl(method = "cv", number = 3),
  ntree = 100
)

# View the best parameters
print(rf_tune_s$bestTune)

```


```{r}
# Calculate variable importance using the trained random forest model
rf_var_imp <- varImp(rf_tune_s, scale = FALSE)

# Plot variable importance using the built-in plot method
plot(rf_var_imp, main = "Variable Importance - Random Forest")

# Alternatively, using ggplot2 for a more customized visualization
library(ggplot2)
var_imp_df <- data.frame(
  Variable = rownames(rf_var_imp$importance),
  Importance = rf_var_imp$importance[, 1]
)

ggplot(var_imp_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(x = "Variables", y = "Importance", title = "Variable Importance - Random Forest") +
  theme_minimal()

```

#XGBoost

```{r}

# XGBoost parameter grid
xgb_grid <- expand.grid(
  nrounds = c(50, 100),
  max_depth = c(4, 6, 8),
  eta = c(0.01, 0.1),
  gamma = 0,              # Fixed gamma
  colsample_bytree = 0.8, # Fixed column sampling
  min_child_weight = 1,   # Fixed minimum child weight
  subsample = 0.8         # Fixed sample sampling
)

# XGBoost cross-validation
set.seed(123)
xgb_tune <- train(
  y ~ ., data = train_data,
  method = "xgbTree",
  tuneGrid = xgb_grid,
  trControl = trainControl(method = "cv", number = 5)
)

# Display the best parameters
print(xgb_tune$bestTune)
```

#use smote
```{r}
# XGBoost parameter grid
xgb_grid_s <- expand.grid(
  nrounds = c(50, 100),
  max_depth = c(4, 6, 8),
  eta = c(0.01, 0.1),
  gamma = 0,              # Fixed gamma
  colsample_bytree = 0.8, # Fixed column sampling
  min_child_weight = 1,   # Fixed minimum child weight
  subsample = 0.8         # Fixed sample sampling
)

# XGBoost cross-validation
set.seed(123)
xgb_tune_s <- train(
  y ~ ., data = train_data_smote,
  method = "xgbTree",
  tuneGrid = xgb_grid_s,
  trControl = trainControl(method = "cv", number = 5)
)

# Display the best parameters
print(xgb_tune_s$bestTune)
```


#GBM


```{r}
# GBM parameter grid
gbm_grid <- expand.grid(
  interaction.depth = c(1, 3, 5),
  n.trees = c(50, 100, 150),
  shrinkage = c(0.01, 0.1),
  n.minobsinnode = 10
)

# GBM cross-validation
set.seed(123)
gbm_tune <- train(
  y ~ ., data = train_data,
  method = "gbm",
  tuneGrid = gbm_grid,
  trControl = trainControl(method = "cv", number = 5),
  verbose = FALSE
)

# Display the best parameters and model performance
print(gbm_tune$bestTune)

```

#use smote

```{r}
# GBM parameter grid
gbm_grid_s <- expand.grid(
  interaction.depth = c(1, 3, 5),
  n.trees = c(50, 100, 150),
  shrinkage = c(0.01, 0.1),
  n.minobsinnode = 10
)

# GBM cross-validation
set.seed(123)
gbm_tune_s <- train(
  y ~ ., data = train_data_smote,
  method = "gbm",
  tuneGrid = gbm_grid_s,
  trControl = trainControl(method = "cv", number = 5),
  verbose = FALSE
)

# Display the best parameters and model performance
print(gbm_tune_s$bestTune)
```




#evaluation indicators
 
#logistic regression

```{r}
# Compute Youden's index
logit_pred_prob <- predict(big_model_no_interaction, newdata = test_data, type = "response")
logit_pred_prob <- as.numeric(logit_pred_prob)
roc_obj <- roc(as.numeric(test_data$y) - 1, logit_pred_prob)
auc_value <- auc(roc_obj)
best_coords <- coords(roc_obj, "best", ret = c("threshold", "specificity", "sensitivity"), best.method = "youden")
print(best_coords)
best_threshold <- as.numeric(best_coords["threshold"])

# Classify predictions using the optimal threshold
logit_pred_class <- ifelse(logit_pred_prob > best_threshold, "yes", "no")
#logit_pred_class <- ifelse(logit_pred_prob > 0.5, "yes", "no")
logit_pred_class <- factor(logit_pred_class, levels = c("no", "yes"))

# Compute confusion matrix
cm <- confusionMatrix(logit_pred_class, test_data$y, positive = "yes")

# Extract evaluation metrics
accuracy <- cm$overall["Accuracy"]
precision <- cm$byClass["Precision"]
recall <- cm$byClass["Recall"]
f1_score <- 2 * (precision * recall) / (precision + recall)
npv <- cm$byClass["Neg Pred Value"]
ppv <- precision  # PPV is Precision

# Compute MSE, MAE, RMSE
mse <- mean((as.numeric(test_data$y) - 1 - logit_pred_prob)^2)
mae <- mean(abs(as.numeric(test_data$y) - 1 - logit_pred_prob))
rmse <- sqrt(mse)

# Store metrics in a list
metrics <- list(
  Accuracy = as.numeric(accuracy),
  Precision = as.numeric(precision),
  Recall = as.numeric(recall),
  F1_Score = as.numeric(f1_score),
  AUC = as.numeric(auc_value),
  PPV = as.numeric(ppv),
  NPV = as.numeric(npv),
  MSE = as.numeric(mse),
  MAE = as.numeric(mae),
  RMSE = as.numeric(rmse)
)

# Output results
print(metrics)
print(cm)

# Explicitly print MSE, MAE, RMSE separately
cat("\nMean Squared Error (MSE):", mse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")

```
 

```{r}
# Compute Youden's index for logistic model with interaction
logit_pred_prob_i <- predict(big_model_i, newdata = test_data, type = "response")
logit_pred_prob_i <- as.numeric(logit_pred_prob_i)
roc_obj_i <- roc(as.numeric(test_data$y) - 1, logit_pred_prob_i)
auc_value_i <- auc(roc_obj_i)
best_coords_i <- coords(roc_obj_i, "best", ret = c("threshold", "specificity", "sensitivity"), best.method = "youden")
print(best_coords_i)
best_threshold_i <- as.numeric(best_coords_i["threshold"])

# Classify predictions using the optimal threshold
logit_pred_class_i <- ifelse(logit_pred_prob_i > best_threshold_i, "yes", "no")
#logit_pred_class_i <- ifelse(logit_pred_prob_i > 0.5, "yes", "no")
logit_pred_class_i <- factor(logit_pred_class_i, levels = c("no", "yes"))

# Compute confusion matrix
cm_i <- confusionMatrix(logit_pred_class_i, test_data$y, positive = "yes")

# Extract evaluation metrics
accuracy_i <- cm_i$overall["Accuracy"]
precision_i <- cm_i$byClass["Precision"]
recall_i <- cm_i$byClass["Recall"]
f1_score_i <- 2 * (precision_i * recall_i) / (precision_i + recall_i)
npv_i <- cm_i$byClass["Neg Pred Value"]
ppv_i <- precision_i  # PPV is Precision

# Compute MSE, MAE, RMSE
mse_i <- mean((as.numeric(test_data$y) - 1 - logit_pred_prob_i)^2)
mae_i <- mean(abs(as.numeric(test_data$y) - 1 - logit_pred_prob_i))
rmse_i <- sqrt(mse_i)

# Store metrics in a list
metrics_i <- list(
  Accuracy = as.numeric(accuracy_i),
  Precision = as.numeric(precision_i),
  Recall = as.numeric(recall_i),
  F1_Score = as.numeric(f1_score_i),
  AUC = as.numeric(auc_value_i),
  PPV = as.numeric(ppv_i),
  NPV = as.numeric(npv_i),
  MSE = as.numeric(mse_i),
  MAE = as.numeric(mae_i),
  RMSE = as.numeric(rmse_i)
)

# Output results
print(metrics_i)
print(cm_i)

# Explicitly print MSE, MAE, RMSE separately
cat("\nMean Squared Error (MSE):", mse_i, "\n")
cat("Mean Absolute Error (MAE):", mae_i, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_i, "\n")

```



#Decision tree




```{r}

# Compute Youden's index
tree_pred_prob <- predict(tree_tune, newdata = test_data, type = "prob")[, "yes"]
roc_obj_tree <- roc(as.numeric(test_data$y) - 1, tree_pred_prob)
auc_value_tree <- auc(roc_obj_tree)
best_coords_tree <- coords(roc_obj_tree, "best", ret = c("threshold", "specificity", "sensitivity"), best.method = "youden")
print(best_coords_tree)
best_threshold_tree <- as.numeric(best_coords_tree["threshold"])

# Classify predictions using the optimal threshold
tree_pred_class <- ifelse(tree_pred_prob > best_threshold_tree, "yes", "no")
#tree_pred_class <- ifelse(tree_pred_prob > 0.5, "yes", "no")
tree_pred_class <- factor(tree_pred_class, levels = c("no", "yes"))

# confusion matrix
cm_tree <- confusionMatrix(tree_pred_class, test_data$y, positive = "yes")

# Extract evaluation metrics
accuracy_tree <- cm_tree$overall["Accuracy"]
precision_tree <- cm_tree$byClass["Precision"]
recall_tree <- cm_tree$byClass["Recall"]
f1_score_tree <- 2 * (precision_tree * recall_tree) / (precision_tree + recall_tree)
npv_tree <- cm_tree$byClass["Neg Pred Value"]
ppv_tree <- precision_tree

# Compute MSE, MAE, RMSE
mse_tree <- mean((as.numeric(test_data$y) - 1 - tree_pred_prob)^2)
mae_tree <- mean(abs(as.numeric(test_data$y) - 1 - tree_pred_prob))
rmse_tree <- sqrt(mse_tree)

# Print metrics
metrics_tree <- list(
  Accuracy = as.numeric(accuracy_tree),
  Precision = as.numeric(precision_tree),
  Recall = as.numeric(recall_tree),
  F1_Score = as.numeric(f1_score_tree),
  AUC = as.numeric(auc_value_tree),
  PPV = as.numeric(ppv_tree),
  NPV = as.numeric(npv_tree),
  MSE = as.numeric(mse_tree),
  MAE = as.numeric(mae_tree),
  RMSE = as.numeric(rmse_tree)
)

print(metrics_tree)
print(cm_tree)

# Explicitly print MSE, MAE, RMSE separately
cat("\nMean Squared Error (MSE):", mse_tree, "\n")
cat("Mean Absolute Error (MAE):", mae_tree, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_tree, "\n")


```

#smote
```{r}
# Compute Youden's index
tree_pred_prob <- predict(tree_tune_s, newdata = test_data, type = "prob")[, "yes"]
roc_obj_tree <- roc(as.numeric(test_data$y) - 1, tree_pred_prob)
auc_value_tree <- auc(roc_obj_tree)
best_coords_tree <- coords(roc_obj_tree, "best", ret = c("threshold", "specificity", "sensitivity"), best.method = "youden")
print(best_coords_tree)
best_threshold_tree <- as.numeric(best_coords_tree["threshold"])

# Classify predictions using the optimal threshold
tree_pred_class <- ifelse(tree_pred_prob > best_threshold_tree, "yes", "no")
#tree_pred_class <- ifelse(tree_pred_prob > 0.5, "yes", "no")
tree_pred_class <- factor(tree_pred_class, levels = c("no", "yes"))

# confusion matrix
cm_tree <- confusionMatrix(tree_pred_class, test_data$y, positive = "yes")

# Extract evaluation metrics
accuracy_tree <- cm_tree$overall["Accuracy"]
precision_tree <- cm_tree$byClass["Precision"]
recall_tree <- cm_tree$byClass["Recall"]
f1_score_tree <- 2 * (precision_tree * recall_tree) / (precision_tree + recall_tree)
npv_tree <- cm_tree$byClass["Neg Pred Value"]
ppv_tree <- precision_tree

# Compute MSE, MAE, RMSE
mse_tree <- mean((as.numeric(test_data$y) - 1 - tree_pred_prob)^2)
mae_tree <- mean(abs(as.numeric(test_data$y) - 1 - tree_pred_prob))
rmse_tree <- sqrt(mse_tree)

# Print metrics
metrics_tree <- list(
  Accuracy = as.numeric(accuracy_tree),
  Precision = as.numeric(precision_tree),
  Recall = as.numeric(recall_tree),
  F1_Score = as.numeric(f1_score_tree),
  AUC = as.numeric(auc_value_tree),
  PPV = as.numeric(ppv_tree),
  NPV = as.numeric(npv_tree),
  MSE = as.numeric(mse_tree),
  MAE = as.numeric(mae_tree),
  RMSE = as.numeric(rmse_tree)
)

print(metrics_tree)
print(cm_tree)

# Explicitly print MSE, MAE, RMSE separately
cat("\nMean Squared Error (MSE):", mse_tree, "\n")
cat("Mean Absolute Error (MAE):", mae_tree, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_tree, "\n")

```

#Random Forest

```{r}

# Compute Youden's index
rf_pred_prob <- predict(rf_tune, newdata = test_data, type = "prob")[, "yes"]
roc_obj_rf <- roc(as.numeric(test_data$y) - 1, rf_pred_prob)
auc_value_rf <- auc(roc_obj_rf)
best_coords_rf <- coords(roc_obj_rf, "best", ret = c("threshold", "specificity", "sensitivity"), best.method = "youden")
print(best_coords_rf)
best_threshold_rf <- as.numeric(best_coords_rf["threshold"])

# Classify predictions using the optimal threshold
rf_pred_class <- ifelse(rf_pred_prob > best_threshold_rf, "yes", "no")
#rf_pred_class <- ifelse(rf_pred_prob > 0.5, "yes", "no")
rf_pred_class <- factor(rf_pred_class, levels = c("no", "yes"))

# confusion matrix
cm_rf <- confusionMatrix(rf_pred_class, test_data$y, positive = "yes")

# Extract evaluation metrics
accuracy_rf <- cm_rf$overall["Accuracy"]
precision_rf <- cm_rf$byClass["Precision"]
recall_rf <- cm_rf$byClass["Recall"]
f1_score_rf <- 2 * (precision_rf * recall_rf) / (precision_rf + recall_rf)
npv_rf <- cm_rf$byClass["Neg Pred Value"]
ppv_rf <- precision_rf

# Compute MSE, MAE, RMSE
mse_rf <- mean((as.numeric(test_data$y) - 1 - rf_pred_prob)^2)
mae_rf <- mean(abs(as.numeric(test_data$y) - 1 - rf_pred_prob))
rmse_rf <- sqrt(mse_rf)

# Print metrics
metrics_rf <- list(
  Accuracy = as.numeric(accuracy_rf),
  Precision = as.numeric(precision_rf),
  Recall = as.numeric(recall_rf),
  F1_Score = as.numeric(f1_score_rf),
  AUC = as.numeric(auc_value_rf),
  PPV = as.numeric(ppv_rf),
  NPV = as.numeric(npv_rf),
  MSE = as.numeric(mse_rf),
  MAE = as.numeric(mae_rf),
  RMSE = as.numeric(rmse_rf)
)

print(metrics_rf)
print(cm_rf)

# Explicitly print MSE, MAE, RMSE separately
cat("\nMean Squared Error (MSE):", mse_rf, "\n")
cat("Mean Absolute Error (MAE):", mae_rf, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_rf, "\n")


```

#smote
```{r}
# Compute Youden's index
rf_pred_prob <- predict(rf_tune_s, newdata = test_data, type = "prob")[, "yes"]
roc_obj_rf <- roc(as.numeric(test_data$y) - 1, rf_pred_prob)
auc_value_rf <- auc(roc_obj_rf)
best_coords_rf <- coords(roc_obj_rf, "best", ret = c("threshold", "specificity", "sensitivity"), best.method = "youden")
print(best_coords_rf)
best_threshold_rf <- as.numeric(best_coords_rf["threshold"])

# Classify predictions using the optimal threshold
rf_pred_class <- ifelse(rf_pred_prob > best_threshold_rf, "yes", "no")
#rf_pred_class <- ifelse(rf_pred_prob > 0.5, "yes", "no")
rf_pred_class <- factor(rf_pred_class, levels = c("no", "yes"))

# confusion matrix
cm_rf <- confusionMatrix(rf_pred_class, test_data$y, positive = "yes")

# Extract evaluation metrics
accuracy_rf <- cm_rf$overall["Accuracy"]
precision_rf <- cm_rf$byClass["Precision"]
recall_rf <- cm_rf$byClass["Recall"]
f1_score_rf <- 2 * (precision_rf * recall_rf) / (precision_rf + recall_rf)
npv_rf <- cm_rf$byClass["Neg Pred Value"]
ppv_rf <- precision_rf

# Compute MSE, MAE, RMSE
mse_rf <- mean((as.numeric(test_data$y) - 1 - rf_pred_prob)^2)
mae_rf <- mean(abs(as.numeric(test_data$y) - 1 - rf_pred_prob))
rmse_rf <- sqrt(mse_rf)

# Print metrics
metrics_rf <- list(
  Accuracy = as.numeric(accuracy_rf),
  Precision = as.numeric(precision_rf),
  Recall = as.numeric(recall_rf),
  F1_Score = as.numeric(f1_score_rf),
  AUC = as.numeric(auc_value_rf),
  PPV = as.numeric(ppv_rf),
  NPV = as.numeric(npv_rf),
  MSE = as.numeric(mse_rf),
  MAE = as.numeric(mae_rf),
  RMSE = as.numeric(rmse_rf)
)

print(metrics_rf)
print(cm_rf)

# Explicitly print MSE, MAE, RMSE separately
cat("\nMean Squared Error (MSE):", mse_rf, "\n")
cat("Mean Absolute Error (MAE):", mae_rf, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_rf, "\n")

```

#XGBoost

```{r}

# Compute Youden's index
xgb_pred_prob <- predict(xgb_tune, newdata = test_data, type = "prob")[, "yes"]
roc_obj_xgb <- roc(as.numeric(test_data$y) - 1, xgb_pred_prob)
auc_value_xgb <- auc(roc_obj_xgb)
best_coords_xgb <- coords(roc_obj_xgb, "best", ret = c("threshold", "specificity", "sensitivity"), best.method = "youden")
print(best_coords_xgb)
best_threshold_xgb <- as.numeric(best_coords_xgb["threshold"])

# Classify predictions using the optimal threshold
xgb_pred_class <- ifelse(xgb_pred_prob > best_threshold_xgb, "yes", "no")
#xgb_pred_class <- ifelse(xgb_pred_prob > 0.5, "yes", "no")
xgb_pred_class <- factor(xgb_pred_class, levels = c("no", "yes"))

# confusion matrix
cm_xgb <- confusionMatrix(xgb_pred_class, test_data$y, positive = "yes")

# Extract evaluation metrics
accuracy_xgb <- cm_xgb$overall["Accuracy"]
precision_xgb <- cm_xgb$byClass["Precision"]
recall_xgb <- cm_xgb$byClass["Recall"]
f1_score_xgb <- 2 * (precision_xgb * recall_xgb) / (precision_xgb + recall_xgb)
npv_xgb <- cm_xgb$byClass["Neg Pred Value"]
ppv_xgb <- precision_xgb

# Compute MSE, MAE, RMSE
mse_xgb <- mean((as.numeric(test_data$y) - 1 - xgb_pred_prob)^2)
mae_xgb <- mean(abs(as.numeric(test_data$y) - 1 - xgb_pred_prob))
rmse_xgb <- sqrt(mse_xgb)

# Print metrics
metrics_xgb <- list(
  Accuracy = as.numeric(accuracy_xgb),
  Precision = as.numeric(precision_xgb),
  Recall = as.numeric(recall_xgb),
  F1_Score = as.numeric(f1_score_xgb),
  AUC = as.numeric(auc_value_xgb),
  PPV = as.numeric(ppv_xgb),
  NPV = as.numeric(npv_xgb),
  MSE = as.numeric(mse_xgb),
  MAE = as.numeric(mae_xgb),
  RMSE = as.numeric(rmse_xgb)
)

print(metrics_xgb)
print(cm_xgb)

# Explicitly print MSE, MAE, RMSE separately
cat("\nMean Squared Error (MSE):", mse_xgb, "\n")
cat("Mean Absolute Error (MAE):", mae_xgb, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_xgb, "\n")


```

#smote
```{r}

# Compute Youden's index
xgb_pred_prob <- predict(xgb_tune_s, newdata = test_data, type = "prob")[, "yes"]
roc_obj_xgb <- roc(as.numeric(test_data$y) - 1, xgb_pred_prob)
auc_value_xgb <- auc(roc_obj_xgb)
best_coords_xgb <- coords(roc_obj_xgb, "best", ret = c("threshold", "specificity", "sensitivity"), best.method = "youden")
print(best_coords_xgb)
best_threshold_xgb <- as.numeric(best_coords_xgb["threshold"])

# Classify predictions using the optimal threshold
xgb_pred_class <- ifelse(xgb_pred_prob > best_threshold_xgb, "yes", "no")
#xgb_pred_class <- ifelse(xgb_pred_prob > 0.5, "yes", "no")
xgb_pred_class <- factor(xgb_pred_class, levels = c("no", "yes"))

# confusion matrix
cm_xgb <- confusionMatrix(xgb_pred_class, test_data$y, positive = "yes")

# Extract evaluation metrics
accuracy_xgb <- cm_xgb$overall["Accuracy"]
precision_xgb <- cm_xgb$byClass["Precision"]
recall_xgb <- cm_xgb$byClass["Recall"]
f1_score_xgb <- 2 * (precision_xgb * recall_xgb) / (precision_xgb + recall_xgb)
npv_xgb <- cm_xgb$byClass["Neg Pred Value"]
ppv_xgb <- precision_xgb

# Compute MSE, MAE, RMSE
mse_xgb <- mean((as.numeric(test_data$y) - 1 - xgb_pred_prob)^2)
mae_xgb <- mean(abs(as.numeric(test_data$y) - 1 - xgb_pred_prob))
rmse_xgb <- sqrt(mse_xgb)

# Print metrics
metrics_xgb <- list(
  Accuracy = as.numeric(accuracy_xgb),
  Precision = as.numeric(precision_xgb),
  Recall = as.numeric(recall_xgb),
  F1_Score = as.numeric(f1_score_xgb),
  AUC = as.numeric(auc_value_xgb),
  PPV = as.numeric(ppv_xgb),
  NPV = as.numeric(npv_xgb),
  MSE = as.numeric(mse_xgb),
  MAE = as.numeric(mae_xgb),
  RMSE = as.numeric(rmse_xgb)
)

print(metrics_xgb)
print(cm_xgb)

# Explicitly print MSE, MAE, RMSE separately
cat("\nMean Squared Error (MSE):", mse_xgb, "\n")
cat("Mean Absolute Error (MAE):", mae_xgb, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_xgb, "\n")


```

#GBM

```{r}

# Compute Youden's index
gbm_pred_prob <- predict(gbm_tune, newdata = test_data, type = "prob")[, "yes"]
roc_obj_gbm <- roc(as.numeric(test_data$y) - 1, gbm_pred_prob)
auc_value_gbm <- auc(roc_obj_gbm)
best_coords_gbm <- coords(roc_obj_gbm, "best", ret = c("threshold", "specificity", "sensitivity"), best.method = "youden")
print(best_coords_gbm)
best_threshold_gbm <- as.numeric(best_coords_gbm["threshold"])

# Classify predictions using the optimal threshold
gbm_pred_class <- ifelse(gbm_pred_prob > best_threshold_gbm, "yes", "no")
#gbm_pred_class <- ifelse(gbm_pred_prob > 0.5, "yes", "no")
gbm_pred_class <- factor(gbm_pred_class, levels = c("no", "yes"))

# confusion matrix
cm_gbm <- confusionMatrix(gbm_pred_class, test_data$y, positive = "yes")

# Extract evaluation metrics
accuracy_gbm <- cm_gbm$overall["Accuracy"]
precision_gbm <- cm_gbm$byClass["Precision"]
recall_gbm <- cm_gbm$byClass["Recall"]
f1_score_gbm <- 2 * (precision_gbm * recall_gbm) / (precision_gbm + recall_gbm)
npv_gbm <- cm_gbm$byClass["Neg Pred Value"]
ppv_gbm <- precision_gbm

# Compute MSE, MAE, RMSE
mse_gbm <- mean((as.numeric(test_data$y) - 1 - gbm_pred_prob)^2)
mae_gbm <- mean(abs(as.numeric(test_data$y) - 1 - gbm_pred_prob))
rmse_gbm <- sqrt(mse_gbm)

# Print metrics
metrics_gbm <- list(
  Accuracy = as.numeric(accuracy_gbm),
  Precision = as.numeric(precision_gbm),
  Recall = as.numeric(recall_gbm),
  F1_Score = as.numeric(f1_score_gbm),
  AUC = as.numeric(auc_value_gbm),
  PPV = as.numeric(ppv_gbm),
  NPV = as.numeric(npv_gbm),
  MSE = as.numeric(mse_gbm),
  MAE = as.numeric(mae_gbm),
  RMSE = as.numeric(rmse_gbm)
)

print(metrics_gbm)
print(cm_gbm)

# Explicitly print MSE, MAE, RMSE separately
cat("\nMean Squared Error (MSE):", mse_gbm, "\n")
cat("Mean Absolute Error (MAE):", mae_gbm, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_gbm, "\n")

```

#smote
```{r}
# Compute Youden's index
gbm_pred_prob <- predict(gbm_tune_s, newdata = test_data, type = "prob")[, "yes"]
roc_obj_gbm <- roc(as.numeric(test_data$y) - 1, gbm_pred_prob)
auc_value_gbm <- auc(roc_obj_gbm)
best_coords_gbm <- coords(roc_obj_gbm, "best", ret = c("threshold", "specificity", "sensitivity"), best.method = "youden")
print(best_coords_gbm)
best_threshold_gbm <- as.numeric(best_coords_gbm["threshold"])

# Classify predictions using the optimal threshold
gbm_pred_class <- ifelse(gbm_pred_prob > best_threshold_gbm, "yes", "no")
#gbm_pred_class <- ifelse(gbm_pred_prob > 0.5, "yes", "no")
gbm_pred_class <- factor(gbm_pred_class, levels = c("no", "yes"))

# confusion matrix
cm_gbm <- confusionMatrix(gbm_pred_class, test_data$y, positive = "yes")

# Extract evaluation metrics
accuracy_gbm <- cm_gbm$overall["Accuracy"]
precision_gbm <- cm_gbm$byClass["Precision"]
recall_gbm <- cm_gbm$byClass["Recall"]
f1_score_gbm <- 2 * (precision_gbm * recall_gbm) / (precision_gbm + recall_gbm)
npv_gbm <- cm_gbm$byClass["Neg Pred Value"]
ppv_gbm <- precision_gbm

# Compute MSE, MAE, RMSE
mse_gbm <- mean((as.numeric(test_data$y) - 1 - gbm_pred_prob)^2)
mae_gbm <- mean(abs(as.numeric(test_data$y) - 1 - gbm_pred_prob))
rmse_gbm <- sqrt(mse_gbm)

# Print metrics
metrics_gbm <- list(
  Accuracy = as.numeric(accuracy_gbm),
  Precision = as.numeric(precision_gbm),
  Recall = as.numeric(recall_gbm),
  F1_Score = as.numeric(f1_score_gbm),
  AUC = as.numeric(auc_value_gbm),
  PPV = as.numeric(ppv_gbm),
  NPV = as.numeric(npv_gbm),
  MSE = as.numeric(mse_gbm),
  MAE = as.numeric(mae_gbm),
  RMSE = as.numeric(rmse_gbm)
)

print(metrics_gbm)
print(cm_gbm)

# Explicitly print MSE, MAE, RMSE separately
cat("\nMean Squared Error (MSE):", mse_gbm, "\n")
cat("Mean Absolute Error (MAE):", mae_gbm, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_gbm, "\n")

```

#ROC curve(combine use with best logistics model)


```{r}
# Logistic Regression
logit_pred_prob <- predict(big_model_i, newdata = test_data, type = "response")

# Decision Tree
tree_pred_prob <- predict(tree_tune, newdata = test_data, type = "prob")[, "yes"]

# Random Forest
rf_pred_prob <- predict(rf_tune, newdata = test_data, type = "prob")[, "yes"]

# XGBoost
xgb_pred_prob <- predict(xgb_tune, newdata = test_data, type = "prob")[, "yes"]

# GBM
gbm_pred_prob <- predict(gbm_tune, newdata = test_data, type = "prob")[, "yes"]

# Convert dependent variable to 0/1
test_y_numeric <- as.numeric(test_data$y) - 1

# Compute ROC curves
roc_logit <- roc(test_y_numeric, logit_pred_prob, levels = c(0, 1), direction = "<")
roc_tree <- roc(test_y_numeric, tree_pred_prob, levels = c(0, 1), direction = "<")
roc_rf <- roc(test_y_numeric, rf_pred_prob, levels = c(0, 1), direction = "<")
roc_xgb <- roc(test_y_numeric, xgb_pred_prob, levels = c(0, 1), direction = "<")
roc_gbm <- roc(test_y_numeric, gbm_pred_prob, levels = c(0, 1), direction = "<")

# Plot ROC curve for Logistic Regression
plot.roc(roc_logit, col = "blue", main = "ROC Curves for All Models", lwd = 2)

# Add ROC curves for other models
lines.roc(roc_tree, col = "green", lwd = 2, print.auc = TRUE, print.auc.y = 0.4)
lines.roc(roc_rf, col = "red", lwd = 2, print.auc = TRUE, print.auc.y = 0.3)
lines.roc(roc_xgb, col = "purple", lwd = 2, print.auc = TRUE, print.auc.y = 0.2)
lines.roc(roc_gbm, col = "orange", lwd = 2, print.auc = TRUE, print.auc.y = 0.1)

# Add legend
legend("bottomright", legend = c("Logistic Regression", "Decision Tree", "Random Forest", "XGBoost", "GBM"),
       col = c("blue", "green", "red", "purple", "orange"), lwd = 2)
```


```{r}
# Set up plotting area for 2 rows and 3 columns
par(mfrow = c(1, 1))

# Plot ROC curve for Logistic Regression
plot.roc(roc_logit, col = "blue", main = "ROC Curve: Logistic Regression", lwd = 1.5)
text(0.5, 0.2, "Logistic Regression", col = "black", cex = 1.0, font = 2)

# Plot ROC curve for Decision Tree
plot.roc(roc_tree, col = "blue", main = "ROC Curve: Decision Tree", lwd = 1.5)
text(0.5, 0.2, "Decision Tree", col = "black", cex = 1.0, font = 2)

# Plot ROC curve for Random Forest
plot.roc(roc_rf, col = "blue", main = "ROC Curve: Random Forest", lwd = 1.5)
text(0.5, 0.2, "Random Forest", col = "black", cex = 1.0, font = 2)

# Plot ROC curve for XGBoost
plot.roc(roc_xgb, col = "blue", main = "ROC Curve: XGBoost", lwd = 1.5)
text(0.5, 0.2, "XGBoost", col = "black", cex = 1.0, font = 2)

# Plot ROC curve for GBM
plot.roc(roc_gbm, col = "blue", main = "ROC Curve: GBM", lwd = 1.5)
text(0.5, 0.2, "GBM", col = "black", cex = 1.0, font = 2)





```
#smote Roc curve
```{r}
# Logistic Regression
logit_pred_prob_s <- predict(big_model_i_s, newdata = test_data, type = "response")

# Decision Tree
tree_pred_prob_s <- predict(tree_tune_s, newdata = test_data, type = "prob")[, "yes"]

# Random Forest
rf_pred_prob_s <- predict(rf_tune_s, newdata = test_data, type = "prob")[, "yes"]

# XGBoost
xgb_pred_prob_s <- predict(xgb_tune_s, newdata = test_data, type = "prob")[, "yes"]

# GBM
gbm_pred_prob_s <- predict(gbm_tune_s, newdata = test_data, type = "prob")[, "yes"]

# Convert dependent variable to 0/1
test_y_numeric <- as.numeric(test_data$y) - 1

# Compute ROC curves
roc_logit_s <- roc(test_y_numeric, logit_pred_prob_s, levels = c(0, 1), direction = "<")
roc_tree_s <- roc(test_y_numeric, tree_pred_prob_s, levels = c(0, 1), direction = "<")
roc_rf_s <- roc(test_y_numeric, rf_pred_prob_s, levels = c(0, 1), direction = "<")
roc_xgb_s <- roc(test_y_numeric, xgb_pred_prob_s, levels = c(0, 1), direction = "<")
roc_gbm_s <- roc(test_y_numeric, gbm_pred_prob_s, levels = c(0, 1), direction = "<")

# Plot ROC curve for Logistic Regression
plot.roc(roc_logit_s, col = "blue", main = "ROC Curves for All Models", lwd = 2)

# Add ROC curves for other models
lines.roc(roc_tree_s, col = "green", lwd = 2, print.auc = TRUE, print.auc.y = 0.4)
lines.roc(roc_rf_s, col = "red", lwd = 2, print.auc = TRUE, print.auc.y = 0.3)
lines.roc(roc_xgb_s, col = "purple", lwd = 2, print.auc = TRUE, print.auc.y = 0.2)
lines.roc(roc_gbm_s, col = "orange", lwd = 2, print.auc = TRUE, print.auc.y = 0.1)

# Add legend
legend("bottomright", legend = c("Logistic Regression", "Decision Tree", "Random Forest", "XGBoost", "GBM"),
       col = c("blue", "green", "red", "purple", "orange"), lwd = 2)
```


```{r}
# Plot ROC curve for Logistic Regression_s
plot.roc(roc_logit_s, col = "blue", main = "ROC Curve: Logistic Regression(smote)", lwd = 1.5)
text(0.5, 0.2, "Logistic Regression", col = "black", cex = 1.0, font = 2)

# Plot ROC curve for Decision Tree_s
plot.roc(roc_tree_s, col = "blue", main = "ROC Curve: Decision Tree(smote)", lwd = 1.5)
text(0.5, 0.2, "Decision Tree", col = "black", cex = 1.0, font = 2)

# Plot ROC curve for Random Forest_s
plot.roc(roc_rf_s, col = "blue", main = "ROC Curve: Random Forest(smote)", lwd = 1.5)
text(0.5, 0.2, "Random Forest", col = "black", cex = 1.0, font = 2)

# Plot ROC curve for XGBoost_s
plot.roc(roc_xgb_s, col = "blue", main = "ROC Curve: XGBoost(smote)", lwd = 1.5)
text(0.5, 0.2, "XGBoost", col = "black", cex = 1.0, font = 2)

# Plot ROC curve for GBM_s
plot.roc(roc_gbm_s, col = "blue", main = "ROC Curve: GBM(smote)", lwd = 1.5)
text(0.5, 0.2, "GBM", col = "black", cex = 1.0, font = 2)
```


#SHAP
(Shapley Additive Explanations) values help interpret the model by measuring how much each feature contributes to a prediction.
```{r}

# XGBoost Modeling

# Convert factor columns to numeric
train_matrix <- model.matrix(y ~ . - 1, data = train_data)
train_label <- as.numeric(train_data$y) - 1

# Create DMatrix
train_dmatrix <- xgb.DMatrix(data = train_matrix, label = train_label)

# Train XGBoost model
xgb_model <- xgboost(
  data = train_dmatrix,
  max.depth = 6,
  eta = 0.1,
  nrounds = 100,
  objective = "binary:logistic",
  eval_metric = "auc"
)

# Convert test set
test_matrix <- model.matrix(y ~ . - 1, data = test_data)

# Compute SHAP values
shp <- shapviz(xgb_model, X_pred = test_matrix, X = test_matrix)

# Plot SHAP summary plot
sv_importance(shp, kind = "beeswarm")

# Plot feature importance
sv_importance(shp, fill = "#0085FF")


```

#numeric value of shap
```{r}
library(xgboost)
library(shapviz)
library(data.table)
library(DT)


shp <- shapviz(xgb_model, X_pred = test_matrix, X = test_matrix)


shap_values <- as.data.frame(shp$S)


shap_means <- colMeans(abs(shap_values))


shap_summary <- data.frame(Feature = names(shap_means), Mean_SHAP = shap_means)
shap_summary <- shap_summary[order(-shap_summary$Mean_SHAP), ]

# pick first 15 characteristics
shap_top15 <- shap_summary[1:15, ]


print(shap_top15)


datatable(shap_top15, options = list(pageLength = 15))

```



```{r}
print(length(y_test))
print(length(final_pred_prob))


```



